{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, images_path, transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.images_path = images_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.dataframe.loc[idx, 'isic_id']\n",
    "        image = Image.open(f'{self.images_path}/{image_path}.jpg').convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, int(self.dataframe.loc[self.dataframe.index[idx], 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "class ISICDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dataframe = pd.read_csv(config.annotations_path)\n",
    "        self.dataframe['path'] = config.path\n",
    "        self.dataframe['target_name'] = config.target_name\n",
    "        self.transform = {\n",
    "            'train': transforms.Compose(\n",
    "                    [\n",
    "                    transforms.RandomResizedCrop(config.img_size, scale=config.crop_scale, antialias=True),\n",
    "                    transforms.Resize((self.config.img_size, self.config.img_size), interpolation=Image.BILINEAR),\n",
    "                    transforms.ToTensor(),\n",
    "                    #transforms.Lambda(lambda x: x / 255.0),\n",
    "                    #transforms.Normalize(mean, std)\n",
    "                    ]),\n",
    "            'test': transforms.Compose(\n",
    "                    [\n",
    "                    #transforms.RandomResizedCrop(config.img_size ,scale=config.crop_scale_tta, antialias=True),\n",
    "                    transforms.Resize((self.config.img_size, self.config.img_size), interpolation=Image.BILINEAR),\n",
    "                    transforms.ToTensor(),\n",
    "                    #transforms.Lambda(lambda x: x / 255.0),\n",
    "                    \n",
    "                    #transforms.Normalize(mean, std)\n",
    "                    ]),\n",
    "            'test_tta':transforms.Compose(\n",
    "                    [\n",
    "                    transforms.RandomResizedCrop(config.img_size, scale=config.crop_scale_tta, antialias=True),\n",
    "                    transforms.Resize((self.config.img_size, self.config.img_size), interpolation=Image.BILINEAR),\n",
    "                    transforms.ToTensor(),\n",
    "                    #transforms.Lambda(lambda x: x / 255.0),\n",
    "                    #transforms.Normalize(mean, std)\n",
    "                    ])\n",
    "                }  # Define your transforms here\n",
    "\n",
    "    def add_label_noise(self, labels, noise_level):\n",
    "        \"\"\"\n",
    "        Adds noise to the labels based on the noise_level.\n",
    "        noise_level: Probability of flipping the label.\n",
    "        \"\"\"\n",
    "        noisy_labels = labels.copy()\n",
    "        for i in range(len(noisy_labels)):\n",
    "            if np.random.rand() < noise_level:\n",
    "                noisy_labels[i] = 1 - noisy_labels[i]  # Flip the label\n",
    "        return noisy_labels\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if self.config.bagging == True:\n",
    "            self.dataframe = self.dataframe.sample(n=self.config.bagging_size, replace=True, random_state=self.config.seed, ignore_index=True).reset_index(drop=True)\n",
    "            \n",
    "        # Apply label noise to the training dataset\n",
    "        if self.config.noise > 0:\n",
    "            self.dataframe['target'] = self.add_label_noise(self.dataframe['target'].values, self.config.noise)\n",
    "            \n",
    "        train_val_df, test_df = train_test_split(self.dataframe, test_size=self.config.test_size, random_state=self.config.seed, stratify=self.dataframe['target'])\n",
    "        train_df, val_df = train_test_split(train_val_df, test_size=self.config.val_size / (self.config.train_size + self.config.val_size), random_state=self.config.seed, stratify=train_val_df['target'])\n",
    "        self.train_dataset = CustomImageDataset(train_df, self.config.path, self.transform['train'])\n",
    "        self.val_dataset = CustomImageDataset(val_df, self.config.path, self.transform['test'])\n",
    "        \n",
    "        \n",
    "        # Fixed test set\n",
    "        if self.config.fixed == True:\n",
    "            test_df = pd.read_csv('/repo/uncertainty_skin/data/isic_balanced/test.csv').reset_index(drop=True)\n",
    "            test_df['path'] = self.config.path\n",
    "            test_df['target_name'] = self.config.target_name        \n",
    "        \n",
    "        self.test_dataset = CustomImageDataset(test_df, self.config.path, self.transform['test'])\n",
    "        self.test_tta_dataset = CustomImageDataset(test_df, self.config.path, self.transform['test_tta'])\n",
    "        \n",
    "        print(self.config.bagging_size)\n",
    "        print(self.dataframe.shape)\n",
    "        \n",
    "        print(train_df['target'].value_counts())\n",
    "        print(val_df['target'].value_counts())\n",
    "        print(test_df['target'].value_counts())\n",
    "        \n",
    "        print(test_df.index)\n",
    "        \n",
    "        #print(f'Train idx: {train_df.reset_index(drop=True).index}')\n",
    "        #print(f'Dataframe shape: {train_df.shape}')\n",
    "        #print(f'Dataframe head: {train_df.head()}')\n",
    "        #print(f'Dataframe head: {train_df.reset_index(drop=True).head()}')\n",
    "        \n",
    "        labels = [int(label) for _, label in zip(train_df.index, train_df.target)]\n",
    "        class_sample_count = [labels.count(i) for i in [0,1]]\n",
    "        weight = 1. / torch.tensor(class_sample_count, dtype=torch.float)\n",
    "        samples_weight = torch.tensor([weight[t] for t in labels])\n",
    "        self.train_sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        \n",
    "        labels = [int(label) for _, label in zip(val_df.index, val_df.target)]\n",
    "        class_sample_count = [labels.count(i) for i in [0,1]]\n",
    "        weight = 1. / torch.tensor(class_sample_count, dtype=torch.float)\n",
    "        samples_weight = torch.tensor([weight[t] for t in labels])\n",
    "        self.val_sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        \n",
    "        labels = [int(label) for _, label in zip(test_df.index, test_df.target)]\n",
    "        class_sample_count = [labels.count(i) for i in [0,1]]\n",
    "        weight = 1. / torch.tensor(class_sample_count, dtype=torch.float)\n",
    "        samples_weight = torch.tensor([weight[t] for t in labels])\n",
    "        self.test_sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        \n",
    "        self.g = torch.Generator()\n",
    "        self.g.manual_seed(self.config.seed)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.config.batch_size,\n",
    "                          num_workers=self.config.num_workers,\n",
    "                          sampler=self.train_sampler,\n",
    "                          pin_memory=True,\n",
    "                          worker_init_fn=self.config.seed % 2**32,\n",
    "                          generator=self.g)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.config.batch_size,\n",
    "                          num_workers=self.config.num_workers,\n",
    "                          sampler=self.val_sampler,\n",
    "                          pin_memory=True,\n",
    "                          worker_init_fn=self.config.seed % 2**32,\n",
    "                          generator=self.g)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_tta_dataset,\n",
    "                            batch_size=self.config.batch_size,\n",
    "                            num_workers=self.config.num_workers,\n",
    "                            sampler=self.test_sampler,\n",
    "                            pin_memory=True,\n",
    "                            worker_init_fn=self.config.seed % 2**32,\n",
    "                            generator=self.g)\n",
    "    def tta_dataloader(self):\n",
    "        return DataLoader(self.test_dataset,\n",
    "                            batch_size=self.config.batch_size,\n",
    "                            num_workers=self.config.num_workers,\n",
    "                            sampler=self.test_sampler,\n",
    "                            pin_memory=True,\n",
    "                            worker_init_fn=self.config.seed % 2**32,\n",
    "                            generator=self.g)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers, regularizers\n",
    "\n",
    "from src.functional.criterion import UANLLloss\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.utils.metrics import calculate_ece, calculate_accuracy, calculate_f1_score_binary, certain_predictions, accuracy_tta, test_vis_tta, ttac, ttaWeightedPred, hist\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stderr, \n",
    "    level=logging.DEBUG, \n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\"\n",
    ")\n",
    "\n",
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.build_model()\n",
    "        self.loss_fun, self.loss_module_1 = self.build_loss()\n",
    "        self.mining_func = miners.TripletMarginMiner(margin=0.2, distance=distances.CosineSimilarity(), type_of_triplets=\"semihard\")\n",
    "\n",
    "        self.epoch_val_loss = []\n",
    "        self.epoch_train_loss = []\n",
    "\n",
    "    def build_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_loss(self):\n",
    "        if self.config.model.loss_fun == 'TM+UANLL':\n",
    "            distance = distances.CosineSimilarity()\n",
    "            reducer = reducers.ThresholdReducer(low=0)\n",
    "            eRegularizer = regularizers.LpRegularizer()\n",
    "            loss_fun = losses.TripletMarginLoss(margin=self.config.model.margin, distance=distance, reducer=reducer, embedding_regularizer=eRegularizer)\n",
    "            loss_module_1 = UANLLloss(smoothing=self.config.model.label_smoothing)\n",
    "            print('UANLL loss is an additional loss term (module 1)')\n",
    "        elif self.config.model.loss_fun == 'TM+CE':\n",
    "            distance = distances.CosineSimilarity()\n",
    "            reducer = reducers.ThresholdReducer(low=0)\n",
    "            eRegularizer = regularizers.LpRegularizer()\n",
    "            loss_fun = losses.TripletMarginLoss(margin=self.config.model.margin, distance=distance, reducer=reducer, embedding_regularizer=eRegularizer)\n",
    "            loss_module_1 = nn.CrossEntropyLoss(label_smoothing=self.config.model.label_smoothing)\n",
    "            print('CE loss is an additional loss term (module 1)')\n",
    "        elif self.config.model.loss_fun == 'CE':\n",
    "            loss_fun = None\n",
    "            loss_module_1 = nn.CrossEntropyLoss(label_smoothing=self.config.model.label_smoothing)\n",
    "            print('Single CE loss')\n",
    "        elif self.config.model.loss_fun == 'UANLL':\n",
    "            loss_fun = None\n",
    "            loss_module_1 = UANLLloss(smoothing=self.config.model.label_smoothing)\n",
    "            print('Single UANLL loss')\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss function: {self.config.model.loss_fun}\")\n",
    "        return loss_fun, loss_module_1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def lr_lambda(self, epoch, n=150, delay=30, stop_lr=0):\n",
    "        n = self.config.trainer.max_epochs\n",
    "        start_lr = self.config.model.lr\n",
    "        learning_rate = start_lr if epoch < delay else start_lr - (epoch - delay) * (start_lr - stop_lr) / (n - 1 - delay)\n",
    "        return learning_rate / start_lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.config.model.optimizer_name == \"Adam\":\n",
    "            optimizer = optim.AdamW(self.parameters(), **self.config.model.optimizer_hparams)\n",
    "        elif self.config.model.optimizer_name == \"SGD\":\n",
    "            optimizer = optim.SGD(self.parameters(), **self.config.model.optimizer_hparams)\n",
    "        else:\n",
    "            assert False, f'Unknown optimizer: \"{self.config.model.optimizer_name}\"'\n",
    "\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer, self.lr_lambda)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        embeddings = self(x)\n",
    "        if self.loss_fun is not None:\n",
    "            indices_tuple = self.mining_func(embeddings[:, :self.config.model.num_classes], y)\n",
    "            loss = self.loss_fun(embeddings[:, :self.config.model.num_classes], y, indices_tuple)\n",
    "        else:\n",
    "            loss = 0\n",
    "        loss += self.loss_module_1(embeddings, y)\n",
    "        if self.config.model.loss_fun in ['TM+CE', 'CE']:\n",
    "            preds = nn.functional.softmax(embeddings, 1)\n",
    "            acc = (preds.argmax(dim=-1) == y).float().mean()\n",
    "        elif self.config.model.loss_fun in ['TM+UANLL', 'UANLL']:\n",
    "            preds = nn.functional.softmax(embeddings[:, :self.config.model.num_classes], 1)\n",
    "            acc = (preds.argmax(dim=-1) == y).float().mean()\n",
    "        self.log(\"train_loss\", loss.float(), on_epoch=True)\n",
    "        self.log(\"train_acc\", acc, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        embeddings = self(x)\n",
    "        if self.loss_fun is not None:\n",
    "            indices_tuple = self.mining_func(embeddings[:, :self.config.model.num_classes], y)\n",
    "            loss = self.loss_fun(embeddings[:, :self.config.model.num_classes], y, indices_tuple)\n",
    "        else:\n",
    "            loss = 0\n",
    "        loss += self.loss_module_1(embeddings, y)\n",
    "        if self.config.model.loss_fun in ['TM+CE', 'CE']:\n",
    "            preds = nn.functional.softmax(embeddings[:, :self.config.model.num_classes], 1)\n",
    "            acc = (preds.argmax(dim=-1) == y).float().mean()\n",
    "        else:\n",
    "            acc = (embeddings[:, :self.config.model.num_classes].argmax(dim=-1) == y).float().mean()\n",
    "        self.log(\"val_loss\", loss.float(), on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        embeddings = self(x)\n",
    "        if self.config.model.loss_fun in ['TM+CE', 'CE']:\n",
    "            preds = nn.functional.softmax(embeddings[:, :self.config.model.num_classes], 1)\n",
    "            acc = (preds.argmax(dim=-1) == y).float().mean()\n",
    "        else:\n",
    "            acc = (embeddings[:, :self.config.model.num_classes].argmax(dim=-1) == y).float().mean()\n",
    "        print(\"Test set accuracy without TTA (Precision@1) = {}\".format(acc))\n",
    "        self.log(\"test_acc\", acc, on_epoch=True)\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Perform predictions, TTA, ensembling, and weighted predictions at the end of the test epoch.\n",
    "        \"\"\"\n",
    "        # Collect predictions without TTA\n",
    "        test_predictions_no_tta, test_labels_no_tta = self.collect_predictions_no_tta()\n",
    "        self.evaluate_no_tta(test_predictions_no_tta, test_labels_no_tta)\n",
    "\n",
    "        # Perform TTA and ensembling\n",
    "        test_predictions_tta, test_labels_tta, test_confidences_tta, test_uncertainties_tta = self.perform_tta_and_ensembling()\n",
    "        self.evaluate_with_tta(test_predictions_tta, test_labels_tta)\n",
    "\n",
    "        # Save predictions to CSV\n",
    "        self.save_predictions_to_csv(test_predictions_no_tta, test_labels_no_tta, test_predictions_tta, test_labels_tta, test_confidences_tta, test_uncertainties_tta)\n",
    "\n",
    "        # Handle weighted predictions based on confidence and certainty\n",
    "        self.handle_weighted_predictions(test_predictions_tta, test_labels_tta, test_confidences_tta, test_uncertainties_tta)\n",
    "\n",
    "        # Compare metrics for different approaches\n",
    "        self.compare_metrics(test_predictions_no_tta, test_labels_no_tta, test_predictions_tta, test_labels_tta, test_confidences_tta, test_uncertainties_tta)\n",
    "\n",
    "    def compare_metrics(self, test_predictions_no_tta: torch.Tensor, test_labels_no_tta: torch.Tensor, test_predictions_tta: torch.Tensor, test_labels_tta: torch.Tensor, test_confidences_tta: torch.Tensor, test_uncertainties_tta: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Compare metrics for different approaches:\n",
    "        - Without TTA\n",
    "        - TTAM (Mode-based TTA)\n",
    "        - TTAWCo-S (Weighted with Confidences)\n",
    "        - TTAWCe-S (Weighted with Certainties)\n",
    "        - Ensembling (Simple)\n",
    "        - Ensembling with Confidences\n",
    "        - Ensembling with Certainties\n",
    "        - Ensembling with TTA\n",
    "\n",
    "        Args:\n",
    "            test_predictions_no_tta (torch.Tensor): Predictions without TTA.\n",
    "            test_labels_no_tta (torch.Tensor): True labels without TTA.\n",
    "            test_predictions_tta (torch.Tensor): Predictions with TTA.\n",
    "            test_labels_tta (torch.Tensor): True labels with TTA.\n",
    "            test_confidences_tta (torch.Tensor): Confidences with TTA.\n",
    "            test_uncertainties_tta (torch.Tensor): Uncertainties with TTA.\n",
    "        \"\"\"\n",
    "        #mode_predictions_tta = self.mode_based_tta(test_predictions_tta)\n",
    "        \n",
    "        # Convert tensors to numpy for sklearn metrics\n",
    "        test_labels_no_tta = test_labels_no_tta.cpu().numpy()\n",
    "        test_labels_tta = test_labels_tta.cpu().numpy()\n",
    "        test_predictions_no_tta = test_predictions_no_tta.argmax(dim=1).cpu().numpy()\n",
    "        test_predictions_tta = test_predictions_tta.argmax(dim=1).cpu().numpy()\n",
    "        test_confidences_tta = test_confidences_tta.cpu().numpy()\n",
    "        test_uncertainties_tta = test_uncertainties_tta.cpu().numpy()\n",
    "\n",
    "        # 1. Without TTA\n",
    "        accuracy_no_tta = accuracy_score(test_labels_no_tta, test_predictions_no_tta)\n",
    "        f1_no_tta = f1_score(test_labels_no_tta, test_predictions_no_tta, average='weighted')\n",
    "        roc_auc_no_tta = roc_auc_score(test_labels_no_tta, test_predictions_no_tta, average='weighted', multi_class='ovr')\n",
    "\n",
    "        # 2. TTAM (Mode-based TTA)\n",
    "        # accuracy_ttam = accuracy_score(test_labels_tta, mode_predictions_tta)\n",
    "        # f1_ttam = f1_score(test_labels_tta, mode_predictions_tta, average='weighted')\n",
    "        # roc_auc_ttam = roc_auc_score(test_labels_tta, mode_predictions_tta, average='weighted', multi_class='ovr')\n",
    "        accuracy_ttam = 0\n",
    "        f1_ttam = 0\n",
    "        roc_auc_ttam = 0\n",
    "\n",
    "        # 3. TTAWCo-S (Weighted with Confidences)\n",
    "        weighted_predictions_co = self.weighted_predictions_with_confidence(test_predictions_tta, test_confidences_tta)\n",
    "        accuracy_tta_co = accuracy_score(test_labels_tta, weighted_predictions_co)\n",
    "        f1_tta_co = f1_score(test_labels_tta, weighted_predictions_co, average='weighted')\n",
    "        roc_auc_tta_co = roc_auc_score(test_labels_tta, weighted_predictions_co, average='weighted', multi_class='ovr')\n",
    "\n",
    "        # 4. TTAWCe-S (Weighted with Certainties)\n",
    "        weighted_predictions_ce = self.weighted_predictions_with_certainty(test_predictions_tta, test_uncertainties_tta)\n",
    "        accuracy_tta_ce = accuracy_score(test_labels_tta, weighted_predictions_ce)\n",
    "        f1_tta_ce = f1_score(test_labels_tta, weighted_predictions_ce, average='weighted')\n",
    "        roc_auc_tta_ce = roc_auc_score(test_labels_tta, weighted_predictions_ce, average='weighted', multi_class='ovr')\n",
    "\n",
    "        # 5. Ensembling (Simple)\n",
    "        ensembled_predictions = self.ensemble_predictions(test_predictions_tta)\n",
    "        accuracy_ensemble = accuracy_score(test_labels_tta, ensembled_predictions)\n",
    "        f1_ensemble = f1_score(test_labels_tta, ensembled_predictions, average='weighted')\n",
    "        roc_auc_ensemble = roc_auc_score(test_labels_tta, ensembled_predictions, average='weighted', multi_class='ovr')\n",
    "\n",
    "        # 6. Ensembling with Confidences\n",
    "        ensembled_predictions_co = self.ensemble_predictions_with_confidence(test_predictions_tta, test_confidences_tta)\n",
    "        accuracy_ensemble_co = accuracy_score(test_labels_tta, ensembled_predictions_co)\n",
    "        f1_ensemble_co = f1_score(test_labels_tta, ensembled_predictions_co, average='weighted')\n",
    "        roc_auc_ensemble_co = roc_auc_score(test_labels_tta, ensembled_predictions_co, average='weighted', multi_class='ovr')\n",
    "\n",
    "        # 7. Ensembling with Certainties\n",
    "        ensembled_predictions_ce = self.ensemble_predictions_with_certainty(test_predictions_tta, test_uncertainties_tta)\n",
    "        accuracy_ensemble_ce = accuracy_score(test_labels_tta, ensembled_predictions_ce)\n",
    "        f1_ensemble_ce = f1_score(test_labels_tta, ensembled_predictions_ce, average='weighted')\n",
    "        roc_auc_ensemble_ce = roc_auc_score(test_labels_tta, ensembled_predictions_ce, average='weighted', multi_class='ovr')\n",
    "\n",
    "        # 8. Ensembling with TTA\n",
    "        ensembled_tta_predictions = self.ensemble_tta_predictions(test_predictions_tta)\n",
    "        accuracy_ensemble_tta = accuracy_score(test_labels_tta, ensembled_tta_predictions)\n",
    "        f1_ensemble_tta = f1_score(test_labels_tta, ensembled_tta_predictions, average='weighted')\n",
    "        roc_auc_ensemble_tta = roc_auc_score(test_labels_tta, ensembled_tta_predictions, average='weighted', multi_class='ovr')\n",
    "\n",
    "        # Create a metrics comparison table\n",
    "        metrics_table = pd.DataFrame({\n",
    "            'Approach': ['Without TTA', 'TTAM', 'TTAWCo-S', 'TTAWCe-S', 'Ensembling (Simple)', 'Ensembling with Confidences', 'Ensembling with Certainties', 'Ensembling with TTA'],\n",
    "            'Accuracy': [accuracy_no_tta, accuracy_ttam, accuracy_tta_co, accuracy_tta_ce, accuracy_ensemble, accuracy_ensemble_co, accuracy_ensemble_ce, accuracy_ensemble_tta],\n",
    "            'F1 Score': [f1_no_tta, f1_ttam, f1_tta_co, f1_tta_ce, f1_ensemble, f1_ensemble_co, f1_ensemble_ce, f1_ensemble_tta],\n",
    "            'ROC-AUC': [roc_auc_no_tta, roc_auc_ttam, roc_auc_tta_co, roc_auc_tta_ce, roc_auc_ensemble, roc_auc_ensemble_co, roc_auc_ensemble_ce, roc_auc_ensemble_tta]\n",
    "        })\n",
    "\n",
    "        # Save the metrics table to a CSV file\n",
    "        metrics_table.to_csv(f\"{self.config.model.name}_{self.config.dataset.seed}_metrics_comparison.csv\", index=False)\n",
    "\n",
    "        # Print the metrics table\n",
    "        print(metrics_table)\n",
    "\n",
    "    def mode_based_tta(self, test_predictions_tta: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute mode-based TTA predictions.\n",
    "\n",
    "        Args:\n",
    "            test_predictions_tta (torch.Tensor): Predictions with TTA.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Mode-based TTA predictions.\n",
    "        \"\"\"\n",
    "        return torch.mode(test_predictions_tta, dim=0).values.cpu().numpy()\n",
    "\n",
    "    def weighted_predictions_with_confidence(self, test_predictions_tta: torch.Tensor, test_confidences_tta: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute weighted predictions based on confidence.\n",
    "\n",
    "        Args:\n",
    "            test_predictions_tta (torch.Tensor): Predictions with TTA.\n",
    "            test_confidences_tta (torch.Tensor): Confidences with TTA.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Weighted predictions based on confidence.\n",
    "        \"\"\"\n",
    "        weighted_predictions_co = test_predictions_tta\n",
    "        weighted_predictions_co[test_confidences_tta < 0.5] = -1  # Ignore low-confidence predictions\n",
    "        return weighted_predictions_co\n",
    "    \n",
    "    def weighted_predictions_with_certainty(self, test_predictions_tta: torch.Tensor, test_uncertainties_tta: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute weighted predictions based on certainty.\n",
    "\n",
    "        Args:\n",
    "            test_predictions_tta (torch.Tensor): Predictions with TTA.\n",
    "            test_uncertainties_tta (torch.Tensor): Uncertainties with TTA.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Weighted predictions based on certainty.\n",
    "        \"\"\"\n",
    "        weighted_predictions_ce = test_predictions_tta\n",
    "        weighted_predictions_ce[test_uncertainties_tta > 0.5] = -1  # Ignore high-uncertainty predictions\n",
    "        return weighted_predictions_ce\n",
    "\n",
    "    def ensemble_predictions(self, test_predictions_tta: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute simple ensembled predictions.\n",
    "\n",
    "        Args:\n",
    "            test_predictions_tta (torch.Tensor): Predictions with TTA.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Ensembled predictions.\n",
    "        \"\"\"\n",
    "        return test_predictions_tta.mean(dim=0).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    def ensemble_predictions_with_confidence(self, test_predictions_tta: torch.Tensor, test_confidences_tta: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ensembled predictions weighted by confidence.\n",
    "\n",
    "        Args:\n",
    "            test_predictions_tta (torch.Tensor): Predictions with TTA.\n",
    "            test_confidences_tta (torch.Tensor): Confidences with TTA.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Ensembled predictions weighted by confidence.\n",
    "        \"\"\"\n",
    "        weighted_predictions = test_predictions_tta * test_confidences_tta.unsqueeze(1)\n",
    "        return weighted_predictions.sum(dim=0).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    def ensemble_predictions_with_certainty(self, test_predictions_tta: torch.Tensor, test_uncertainties_tta: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ensembled predictions weighted by certainty.\n",
    "\n",
    "        Args:\n",
    "            test_predictions_tta (torch.Tensor): Predictions with TTA.\n",
    "            test_uncertainties_tta (torch.Tensor): Certainties with TTA.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Ensembled predictions weighted by certainty.\n",
    "        \"\"\"\n",
    "        weighted_predictions = test_predictions_tta * test_uncertainties_tta.unsqueeze(1)\n",
    "        return weighted_predictions.sum(dim=0).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    def ensemble_tta_predictions(self, test_predictions_tta: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ensembled predictions with TTA.\n",
    "\n",
    "        Args:\n",
    "            test_predictions_tta (torch.Tensor): Predictions with TTA.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Ensembled predictions with TTA.\n",
    "        \"\"\"\n",
    "        return test_predictions_tta.mean(dim=0).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    def collect_predictions_no_tta(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Collect predictions without Test-Time Augmentation (TTA).\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: Predictions and labels without TTA.\n",
    "        \"\"\"\n",
    "        test_predictions_no_tta, test_labels_no_tta = [], []\n",
    "        for batch in self.trainer.datamodule.test_dataloader():\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = self(inputs)\n",
    "\n",
    "            # Handle model output as in test_step\n",
    "            if self.config.model.loss_fun in ['TM+CE', 'CE']:\n",
    "                preds = torch.softmax(outputs[:, :self.config.model.num_classes], dim=1)\n",
    "            else:\n",
    "                preds = torch.softmax(outputs[:, :self.config.model.num_classes], dim=1)\n",
    "\n",
    "            test_predictions_no_tta.append(preds)\n",
    "            test_labels_no_tta.append(labels)\n",
    "\n",
    "        test_predictions_no_tta = torch.cat(test_predictions_no_tta)\n",
    "        test_labels_no_tta = torch.cat(test_labels_no_tta)\n",
    "        return test_predictions_no_tta, test_labels_no_tta\n",
    "\n",
    "    def evaluate_no_tta(self, test_predictions_no_tta: torch.Tensor, test_labels_no_tta: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Evaluate predictions without TTA.\n",
    "\n",
    "        Args:\n",
    "            test_predictions_no_tta (torch.Tensor): Predictions without TTA.\n",
    "            test_labels_no_tta (torch.Tensor): True labels without TTA.\n",
    "        \"\"\"\n",
    "        test_predictions_no_tta = test_predictions_no_tta.argmax(dim=1).cpu()\n",
    "        test_labels_no_tta = test_labels_no_tta.squeeze().cpu()\n",
    "\n",
    "        accuracy_no_tta = accuracy_score(test_labels_no_tta, test_predictions_no_tta)\n",
    "        f1_no_tta = f1_score(test_labels_no_tta, test_predictions_no_tta, average='weighted')\n",
    "        roc_auc_no_tta = roc_auc_score(test_labels_no_tta, test_predictions_no_tta, average='weighted', multi_class='ovr')\n",
    "\n",
    "        print(f\"Metrics without TTA: Accuracy={accuracy_no_tta}, F1={f1_no_tta}, ROC-AUC={roc_auc_no_tta}\")\n",
    "\n",
    "    def perform_tta_and_ensembling(self) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Perform Test-Time Augmentation (TTA) and ensembling.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                Ensembled predictions, labels, confidences, and uncertainties.\n",
    "        \"\"\"\n",
    "        num_tta = self.config.dataset.num_tta\n",
    "        checkpoint_paths = list(self.config.model.checkpoint_path)\n",
    "\n",
    "        all_test_predictions_tta = []\n",
    "        all_test_labels_tta = []\n",
    "        all_test_confidences_tta = []\n",
    "        all_test_uncertainties_tta = []\n",
    "\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            # Reinitialize the model and datamodule for each checkpoint\n",
    "            self.build_model()\n",
    "            self.load_state_dict(torch.load(checkpoint_path, weights_only=True)['state_dict'])\n",
    "            self.to(self.device)\n",
    "            self.eval()\n",
    "\n",
    "            datamodule = self.trainer.datamodule  # Reinitialize datamodule if needed\n",
    "            test_loader = datamodule.test_dataloader()\n",
    "\n",
    "            test_predictions_tta, test_labels_tta, test_confidences_tta, test_uncertainties_tta = self.collect_tta_predictions(self, test_loader, num_tta)\n",
    "            all_test_predictions_tta.append(test_predictions_tta)\n",
    "            all_test_labels_tta.append(test_labels_tta)\n",
    "            all_test_confidences_tta.append(test_confidences_tta)\n",
    "            all_test_uncertainties_tta.append(test_uncertainties_tta)\n",
    "\n",
    "        # Ensemble predictions\n",
    "        all_test_predictions_tta = torch.stack(all_test_predictions_tta).mean(dim=0)\n",
    "        all_test_labels_tta = torch.stack(all_test_labels_tta).mode(dim=0).values\n",
    "        all_test_confidences_tta = torch.stack(all_test_confidences_tta).mean(dim=0)\n",
    "        all_test_uncertainties_tta = torch.stack(all_test_uncertainties_tta).mean(dim=0)\n",
    "\n",
    "        return all_test_predictions_tta, all_test_labels_tta, all_test_confidences_tta, all_test_uncertainties_tta\n",
    "\n",
    "    def collect_tta_predictions(self, model: torch.nn.Module, test_loader: DataLoader, num_tta: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Collect predictions with Test-Time Augmentation (TTA) for a single checkpoint.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to use for predictions.\n",
    "            test_loader (DataLoader): The test dataloader.\n",
    "            num_tta (int): Number of TTA iterations.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                Predictions, labels, confidences, and uncertainties with TTA.\n",
    "        \"\"\"\n",
    "        test_predictions_tta = []\n",
    "        test_labels_tta = []\n",
    "        test_confidences_tta = []\n",
    "        test_uncertainties_tta = []\n",
    "\n",
    "        for _ in range(num_tta):\n",
    "            for batch in test_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Handle model output as in test_step\n",
    "                if self.config.model.loss_fun in ['TM+CE', 'CE']:\n",
    "                    preds = torch.softmax(outputs[:, :self.config.model.num_classes], dim=1)\n",
    "                else:\n",
    "                    preds = torch.softmax(outputs[:, :self.config.model.num_classes], dim=1)\n",
    "\n",
    "                confidences = preds.max(dim=1).values\n",
    "                uncertainties = outputs[:, -1] if self.config.model.loss_fun in ['UANLL', 'TM+UANLL'] else torch.zeros_like(confidences)\n",
    "\n",
    "                test_predictions_tta.append(preds)\n",
    "                test_labels_tta.append(labels)\n",
    "                test_confidences_tta.append(confidences)\n",
    "                test_uncertainties_tta.append(uncertainties)\n",
    "\n",
    "        test_predictions_tta = torch.cat(test_predictions_tta)\n",
    "        test_labels_tta = torch.cat(test_labels_tta)\n",
    "        test_confidences_tta = torch.cat(test_confidences_tta)\n",
    "        test_uncertainties_tta = torch.cat(test_uncertainties_tta)\n",
    "\n",
    "        return test_predictions_tta, test_labels_tta, test_confidences_tta, test_uncertainties_tta\n",
    "\n",
    "    def evaluate_with_tta(self, test_predictions_tta: torch.Tensor, test_labels_tta: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Evaluate predictions with TTA and ensembling.\n",
    "\n",
    "        Args:\n",
    "            test_predictions_tta (torch.Tensor): Predictions with TTA.\n",
    "            test_labels_tta (torch.Tensor): True labels with TTA.\n",
    "        \"\"\"\n",
    "        print(test_predictions_tta)\n",
    "        test_predictions_tta = test_predictions_tta.argmax(dim=1).cpu()\n",
    "        test_labels_tta = test_labels_tta.squeeze().cpu()\n",
    "\n",
    "        accuracy_tta = accuracy_score(test_labels_tta, test_predictions_tta)\n",
    "        f1_tta = f1_score(test_labels_tta, test_predictions_tta, average='weighted')\n",
    "        roc_auc_tta = roc_auc_score(test_labels_tta, test_predictions_tta, average='weighted', multi_class='ovr')\n",
    "\n",
    "        print(f\"Metrics with TTA: Accuracy={accuracy_tta}, F1={f1_tta}, ROC-AUC={roc_auc_tta}\")\n",
    "\n",
    "    def save_predictions_to_csv(self, test_predictions_no_tta: torch.Tensor, test_labels_no_tta: torch.Tensor, test_predictions_tta: torch.Tensor, test_labels_tta: torch.Tensor, test_confidences_tta: torch.Tensor, test_uncertainties_tta: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Save predictions (without TTA and with TTA) to CSV files.\n",
    "\n",
    "        Args:\n",
    "            test_predictions_no_tta (torch.Tensor): Predictions without TTA.\n",
    "            test_labels_no_tta (torch.Tensor): True labels without TTA.\n",
    "            test_predictions_tta (torch.Tensor): Predictions with TTA.\n",
    "            test_labels_tta (torch.Tensor): True labels with TTA.\n",
    "            test_confidences_tta (torch.Tensor): Confidences with TTA.\n",
    "            test_uncertainties_tta (torch.Tensor): Uncertainties with TTA.\n",
    "        \"\"\"\n",
    "        # Save predictions without TTA\n",
    "        no_tta_df = pd.DataFrame({\n",
    "            'true_labels': test_labels_no_tta.tolist(),\n",
    "            'predictions': test_predictions_no_tta.argmax(dim=1).tolist()\n",
    "        })\n",
    "        no_tta_df.to_csv(f\"{self.config.model.name}_{self.config.dataset.seed}_predictions_no_tta.csv\", index=False)\n",
    "\n",
    "        # Save predictions with TTA\n",
    "        tta_df = pd.DataFrame({\n",
    "            'true_labels': test_labels_tta.tolist(),\n",
    "            'predictions': test_predictions_tta.argmax(dim=1).tolist(),\n",
    "            'confidences': test_confidences_tta.tolist(),\n",
    "            'uncertainties': test_uncertainties_tta.tolist()\n",
    "        })\n",
    "        tta_df.to_csv(f\"{self.config.model.name}_{self.config.dataset.seed}_predictions_tta.csv\", index=False)\n",
    "\n",
    "    def handle_weighted_predictions(self, test_predictions_tta: torch.Tensor, test_labels_tta: torch.Tensor, test_confidences_tta: torch.Tensor, test_uncertainties_tta: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Handle weighted predictions based on confidence and certainty.\n",
    "\n",
    "        Args:\n",
    "            test_predictions_tta (torch.Tensor): Predictions with TTA.\n",
    "            test_labels_tta (torch.Tensor): True labels with TTA.\n",
    "            test_confidences_tta (torch.Tensor): Confidences with TTA.\n",
    "            test_uncertainties_tta (torch.Tensor): Uncertainties with TTA.\n",
    "        \"\"\"\n",
    "        # Weighted predictions based on confidence\n",
    "        weighted_predictions_co = test_predictions_tta.argmax(dim=1)\n",
    "        weighted_predictions_co[test_confidences_tta < 0.5] = -1  # Example: Ignore low-confidence predictions\n",
    "\n",
    "        # Weighted predictions based on certainty\n",
    "        weighted_predictions_ce = test_predictions_tta.argmax(dim=1)\n",
    "        weighted_predictions_ce[test_uncertainties_tta > 0.5] = -1  # Example: Ignore high-uncertainty predictions\n",
    "\n",
    "        # Evaluate weighted predictions\n",
    "        self.evaluate_weighted_predictions(weighted_predictions_co, weighted_predictions_ce, test_labels_tta)\n",
    "\n",
    "    def evaluate_weighted_predictions(self, weighted_predictions_co: torch.Tensor, weighted_predictions_ce: torch.Tensor, test_labels_tta: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Evaluate weighted predictions based on confidence and certainty.\n",
    "\n",
    "        Args:\n",
    "            weighted_predictions_co (torch.Tensor): Weighted predictions based on confidence.\n",
    "            weighted_predictions_ce (torch.Tensor): Weighted predictions based on certainty.\n",
    "            test_labels_tta (torch.Tensor): True labels with TTA.\n",
    "        \"\"\"\n",
    "        # Filter out ignored predictions\n",
    "        valid_indices_co = weighted_predictions_co != -1\n",
    "        valid_indices_ce = weighted_predictions_ce != -1\n",
    "        \n",
    "        valid_indices_ce = valid_indices_ce.cpu().numpy()\n",
    "        valid_indices_co = valid_indices_co.cpu().numpy()\n",
    "        \n",
    "        test_labels_tta = test_labels_tta.cpu().numpy()\n",
    "        weighted_predictions_co = weighted_predictions_co.cpu().numpy()\n",
    "        weighted_predictions_ce = weighted_predictions_ce.cpu().numpy()\n",
    "\n",
    "        # Evaluate confidence-weighted predictions\n",
    "        accuracy_co = accuracy_score(test_labels_tta[valid_indices_co], weighted_predictions_co[valid_indices_co])\n",
    "        f1_co = f1_score(test_labels_tta[valid_indices_co], weighted_predictions_co[valid_indices_co], average='weighted')\n",
    "        roc_auc_co = roc_auc_score(test_labels_tta[valid_indices_co], weighted_predictions_co[valid_indices_co], average='weighted', multi_class='ovr')\n",
    "\n",
    "        # Evaluate certainty-weighted predictions\n",
    "        accuracy_ce = accuracy_score(test_labels_tta[valid_indices_ce], weighted_predictions_ce[valid_indices_ce])\n",
    "        f1_ce = f1_score(test_labels_tta[valid_indices_ce], weighted_predictions_ce[valid_indices_ce], average='weighted')\n",
    "        roc_auc_ce = roc_auc_score(test_labels_tta[valid_indices_ce], weighted_predictions_ce[valid_indices_ce], average='weighted', multi_class='ovr')\n",
    "\n",
    "        print(f\"Confidence-Weighted Metrics: Accuracy={accuracy_co}, F1={f1_co}, ROC-AUC={roc_auc_co}\")\n",
    "        print(f\"Certainty-Weighted Metrics: Accuracy={accuracy_ce}, F1={f1_ce}, ROC-AUC={roc_auc_ce}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "class TimmModel(BaseModel):\n",
    "    def build_model(self):\n",
    "        if self.config.model.loss_fun in ['TM+UANLL', 'UANLL']:\n",
    "            num_classes = self.config.model.num_classes + 1\n",
    "        else:\n",
    "            num_classes = self.config.model.num_classes\n",
    "\n",
    "        self.model = timm.create_model(self.config.model.name, pretrained=self.config.model.pretrained, num_classes=num_classes)\n",
    "        #self.model.conv1 = nn.Conv2d(self.config.model.input_channel, self.model.conv1.out_channels, kernel_size=self.model.conv1.kernel_size, \n",
    "        #                stride=self.model.conv1.stride, padding=self.model.conv1.padding, bias=False)\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def intermediate_forward(self, x):\n",
    "        # Assuming the last layer before the classifier is the penultimate layer\n",
    "        for name, module in self.model.named_children():\n",
    "            if name == 'head':\n",
    "                break\n",
    "            x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(BaseModel):\n",
    "    def build_model(self):\n",
    "        input_channel = self.config.model.input_channel\n",
    "        if self.config.model.loss_fun in ['TM+UANLL', 'UANLL']:\n",
    "            print('CNN. Uncertainty toggled')\n",
    "            n_outputs = self.config.model.num_classes + 1\n",
    "        else:\n",
    "            n_outputs = self.config.model.num_classes\n",
    "\n",
    "        dropout_rate = self.config.model.dropout_rate\n",
    "        top_bn = self.config.model.top_bn\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.top_bn = top_bn\n",
    "        self.c1 = nn.Conv2d(input_channel, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.c2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.c3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.c4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.c5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.c6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.c7 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=0)\n",
    "        self.c8 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=0)\n",
    "        self.c9 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=0)\n",
    "        self.l_c1 = nn.Linear(128, n_outputs)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.bn8 = nn.BatchNorm2d(256)\n",
    "        self.bn9 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Define batch normalization for the linear layer if needed\n",
    "        if self.top_bn:\n",
    "            self.bn_c1 = nn.BatchNorm1d(n_outputs)\n",
    "\n",
    "    def call_bn(self, bn, x):\n",
    "        return bn(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = self.c1(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn1, h), negative_slope=0.01)\n",
    "        h = self.c2(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn2, h), negative_slope=0.01)\n",
    "        h = self.c3(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn3, h), negative_slope=0.01)\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "        h = F.dropout2d(h, p=self.dropout_rate)\n",
    "\n",
    "        h = self.c4(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn4, h), negative_slope=0.01)\n",
    "        h = self.c5(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn5, h), negative_slope=0.01)\n",
    "        h = self.c6(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn6, h), negative_slope=0.01)\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "        h = F.dropout2d(h, p=self.dropout_rate)\n",
    "\n",
    "        h = self.c7(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn7, h), negative_slope=0.01)\n",
    "        h = self.c8(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn8, h), negative_slope=0.01)\n",
    "        h = self.c9(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn9, h), negative_slope=0.01)\n",
    "        h = F.avg_pool2d(h, kernel_size=h.data.shape[2])\n",
    "\n",
    "        h = h.view(h.size(0), h.size(1))\n",
    "        logit = self.l_c1(h)\n",
    "        if self.top_bn:\n",
    "            logit = self.call_bn(self.bn_c1, logit)\n",
    "        return logit\n",
    "\n",
    "    def intermediate_forward(self, x):\n",
    "        h = x\n",
    "        h = self.c1(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn1, h), negative_slope=0.01)\n",
    "        h = self.c2(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn2, h), negative_slope=0.01)\n",
    "        h = self.c3(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn3, h), negative_slope=0.01)\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "        h = F.dropout2d(h, p=self.dropout_rate)\n",
    "\n",
    "        h = self.c4(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn4, h), negative_slope=0.01)\n",
    "        h = self.c5(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn5, h), negative_slope=0.01)\n",
    "        h = self.c6(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn6, h), negative_slope=0.01)\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "        h = F.dropout2d(h, p=self.dropout_rate)\n",
    "\n",
    "        h = self.c7(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn7, h), negative_slope=0.01)\n",
    "        h = self.c8(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn8, h), negative_slope=0.01)\n",
    "        h = self.c9(h)\n",
    "        h = F.leaky_relu(self.call_bn(self.bn9, h), negative_slope=0.01)\n",
    "        h = F.avg_pool2d(h, kernel_size=h.data.shape[2])\n",
    "\n",
    "        h = h.view(h.size(0), h.size(1))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import pytorch_lightning as pl\n",
    "from src.utils.clearml_logger import ClearMLLogger\n",
    "from src.utils.utils import set_seed\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stderr, \n",
    "    level=logging.DEBUG, \n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\"\n",
    ")\n",
    "\n",
    "def test(config: DictConfig,\n",
    "         seed: int,\n",
    "         checkpoint_path: str,\n",
    "         logger: callable,\n",
    "         unique_id: str):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    data_module = ISICDataModule(config.dataset)\n",
    "    data_module.setup()\n",
    "    \n",
    "    config.dataset.seed = seed\n",
    "\n",
    "    print(f\"Testing model trained on seed: {seed}\")\n",
    "\n",
    "    if config.model.name == 'CNN':\n",
    "        model = CNN.load_from_checkpoint(checkpoint_path, config=config)\n",
    "    elif config.model.name in config.timm_models:\n",
    "        model = TimmModel.load_from_checkpoint(checkpoint_path[0], config=config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {config.model.name}\")\n",
    "\n",
    "    trainer = pl.Trainer(**config.trainer,\n",
    "                         logger=logger,\n",
    "                         deterministic=True)\n",
    "\n",
    "    trainer.test(model, datamodule=data_module)\n",
    "\n",
    "    # Load the summary CSV generated by the model\n",
    "    #summary_df = pd.read_csv(f\"{config.model.name}_{config.dataset.seed}_summary.csv\")\n",
    "    #return summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 16:06:42,876 DEBUG: Setting JobRuntime:name=test_app\n",
      "2024-12-24 16:06:42,917 INFO: Checkpoint path: /repo/uncertainty_skin/multirun/checkpoints_multirun/resnet18_0_TM+CE_61539e93-55c3-43e5-aca5-4e35c95aa493_epoch=99.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "timm_models:\n",
      "- resnet50\n",
      "- resnet18\n",
      "offline: false\n",
      "clearml_model_id: a65ecaf5e7644d8398383cfc67385ad6\n",
      "dataset:\n",
      "  name: isic_balanced\n",
      "  path: /repo/uncertainty_skin/data/isic_balanced/images\n",
      "  annotations_path: /repo/uncertainty_skin/data/isic_balanced/dataset.csv\n",
      "  img_size: 224\n",
      "  crop_scale:\n",
      "  - 0.8\n",
      "  - 1.0\n",
      "  crop_scale_tta:\n",
      "  - 0.8\n",
      "  - 1.0\n",
      "  batch_size: 32\n",
      "  num_workers: 0\n",
      "  target_name: target\n",
      "  class_names:\n",
      "  - benign\n",
      "  - malignant\n",
      "  train_size: 0.7\n",
      "  val_size: 0.15\n",
      "  test_size: 0.15\n",
      "  seed: 42\n",
      "  bagging: true\n",
      "  bagging_size: 15000\n",
      "  num_tta: 100\n",
      "  noise: 0.0\n",
      "  fixed: false\n",
      "model:\n",
      "  name: resnet18\n",
      "  num_classes: 2\n",
      "  pretrained: true\n",
      "  input_channel: 3\n",
      "  dropout_rate: 0.25\n",
      "  top_bn: false\n",
      "  loss_fun: TM+CE\n",
      "  label_smoothing: 0.1\n",
      "  lr: 0.0001\n",
      "  checkpoint_path:\n",
      "  - /repo/uncertainty_skin/multirun/checkpoints_multirun/resnet18_0_TM+CE_61539e93-55c3-43e5-aca5-4e35c95aa493_epoch=99.ckpt\n",
      "  - /repo/uncertainty_skin/multirun/checkpoints_multirun/resnet18_3_TM+CE_61539e93-55c3-43e5-aca5-4e35c95aa493_epoch=89.ckpt\n",
      "  - /repo/uncertainty_skin/multirun/checkpoints_multirun/resnet18_9_TM+CE_61539e93-55c3-43e5-aca5-4e35c95aa493_epoch=98.ckpt\n",
      "  - /repo/uncertainty_skin/multirun/checkpoints_multirun/resnet18_17_TM+CE_61539e93-55c3-43e5-aca5-4e35c95aa493_epoch=99.ckpt\n",
      "  - /repo/uncertainty_skin/multirun/checkpoints_multirun/resnet18_42_TM+CE_61539e93-55c3-43e5-aca5-4e35c95aa493_epoch=92.ckpt\n",
      "  optimizer_name: Adam\n",
      "  num_tta: 10\n",
      "  optimizer_hparams:\n",
      "    weight_decay: 0.0001\n",
      "  margin: 1.2\n",
      "trainer:\n",
      "  max_epochs: 100\n",
      "? ''\n",
      ": ? ''\n",
      "  : ? ''\n",
      "    : model:\n",
      "        name: resnet50\n",
      "        num_classes: 2\n",
      "        pretrained: true\n",
      "        input_channel: 3\n",
      "        dropout_rate: 0.25\n",
      "        top_bn: false\n",
      "        loss_fun: TM+UANLL\n",
      "        label_smoothing: 0.1\n",
      "        lr: 0.0001\n",
      "        checkpoint_path: /repo/uncertainty_skin/outputs/2024-11-01/15-33-48/checkpoints/resnet50_42_TM+UANLL_95fd2a77-b348-4346-835c-57f4c8a3dfed_epoch=1.ckpt\n",
      "        optimizer_name: Adam\n",
      "        num_tta: 10\n",
      "        optimizer_hparams:\n",
      "          weight_decay: 0.0001\n",
      "        margin: 1.2\n",
      "      dataset:\n",
      "        name: isic_balanced\n",
      "        path: /repo/uncertainty_skin/data/isic_balanced/images\n",
      "        annotations_path: /repo/uncertainty_skin/data/isic_balanced/dataset.csv\n",
      "        img_size: 32\n",
      "        crop_scale:\n",
      "        - 0.8\n",
      "        - 1.0\n",
      "        crop_scale_tta:\n",
      "        - 0.8\n",
      "        - 1.0\n",
      "        batch_size: 32\n",
      "        num_workers: 0\n",
      "        target_name: target\n",
      "        class_names:\n",
      "        - benign\n",
      "        - malignant\n",
      "        train_size: 0.7\n",
      "        val_size: 0.15\n",
      "        test_size: 0.15\n",
      "        seed: 42\n",
      "        bagging: true\n",
      "        bagging_size: 15000\n",
      "        num_tta: 100\n",
      "        noise: 0.0\n",
      "        fixed: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from src.utils.clearml_logger import ClearMLLogger\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import uuid\n",
    "import logging \n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "\n",
    "def main():\n",
    "\n",
    "    # global initialization\n",
    "    with initialize(version_base=None, config_path=\"configs\", job_name=\"test_app\"):\n",
    "        config = compose(config_name=\"config\", overrides=[\"+experiment=tm_ce_resnet\"])\n",
    "        print(OmegaConf.to_yaml(config))\n",
    "    \n",
    "    all_predictions_df = pd.DataFrame()\n",
    "    \n",
    "    logging.info(f'Checkpoint path: {config.model.checkpoint_path[0]}')\n",
    "    #checkpoint_path = config.model.checkpoint_path\n",
    "    \n",
    "    #logger = ClearMLLogger(project_name=\"ISIC_2024\",\n",
    "        #                   task_name=f\"{config.model.name}_{seed}_{config.model.loss_fun}_testing\",\n",
    "    #                    offline=config.offline)\n",
    "    \n",
    "    # _, predictions_df = test(config = config,\n",
    "    #                         seed = 42,\n",
    "    #                         checkpoint_path = config.model.checkpoint_path,\n",
    "    #                         logger = None,\n",
    "    #                         unique_id=uuid.uuid4)\n",
    "    \n",
    "    # all_predictions_df = pd.concat([all_predictions_df, predictions_df])\n",
    "                        \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 16:06:42,931 DEBUG: Setting JobRuntime:name=test_app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "timm_models:\n",
      "- resnet50\n",
      "- resnet18\n",
      "offline: false\n",
      "clearml_model_id: a65ecaf5e7644d8398383cfc67385ad6\n",
      "dataset:\n",
      "  name: isic_balanced\n",
      "  path: /repo/uncertainty_skin/data/isic_balanced/images\n",
      "  annotations_path: /repo/uncertainty_skin/data/isic_balanced/dataset.csv\n",
      "  img_size: 224\n",
      "  crop_scale:\n",
      "  - 0.8\n",
      "  - 1.0\n",
      "  crop_scale_tta:\n",
      "  - 0.8\n",
      "  - 1.0\n",
      "  batch_size: 32\n",
      "  num_workers: 0\n",
      "  target_name: target\n",
      "  class_names:\n",
      "  - benign\n",
      "  - malignant\n",
      "  train_size: 0.7\n",
      "  val_size: 0.15\n",
      "  test_size: 0.15\n",
      "  seed: 42\n",
      "  bagging: true\n",
      "  bagging_size: 15000\n",
      "  num_tta: 100\n",
      "  noise: 0.0\n",
      "  fixed: false\n",
      "model:\n",
      "  name: resnet18\n",
      "  num_classes: 2\n",
      "  pretrained: true\n",
      "  input_channel: 3\n",
      "  dropout_rate: 0.25\n",
      "  top_bn: false\n",
      "  loss_fun: TM+CE\n",
      "  label_smoothing: 0.1\n",
      "  lr: 0.0001\n",
      "  checkpoint_path:\n",
      "  - /repo/uncertainty_skin/multirun/checkpoints_multirun/resnet18_0_TM+CE_61539e93-55c3-43e5-aca5-4e35c95aa493_epoch=99.ckpt\n",
      "  - /repo/uncertainty_skin/multirun/checkpoints_multirun/resnet18_3_TM+CE_61539e93-55c3-43e5-aca5-4e35c95aa493_epoch=89.ckpt\n",
      "  - /repo/uncertainty_skin/multirun/checkpoints_multirun/resnet18_9_TM+CE_61539e93-55c3-43e5-aca5-4e35c95aa493_epoch=98.ckpt\n",
      "  - /repo/uncertainty_skin/multirun/checkpoints_multirun/resnet18_17_TM+CE_61539e93-55c3-43e5-aca5-4e35c95aa493_epoch=99.ckpt\n",
      "  - /repo/uncertainty_skin/multirun/checkpoints_multirun/resnet18_42_TM+CE_61539e93-55c3-43e5-aca5-4e35c95aa493_epoch=92.ckpt\n",
      "  optimizer_name: Adam\n",
      "  num_tta: 10\n",
      "  optimizer_hparams:\n",
      "    weight_decay: 0.0001\n",
      "  margin: 1.2\n",
      "trainer:\n",
      "  max_epochs: 100\n",
      "? ''\n",
      ": ? ''\n",
      "  : ? ''\n",
      "    : model:\n",
      "        name: resnet50\n",
      "        num_classes: 2\n",
      "        pretrained: true\n",
      "        input_channel: 3\n",
      "        dropout_rate: 0.25\n",
      "        top_bn: false\n",
      "        loss_fun: TM+UANLL\n",
      "        label_smoothing: 0.1\n",
      "        lr: 0.0001\n",
      "        checkpoint_path: /repo/uncertainty_skin/outputs/2024-11-01/15-33-48/checkpoints/resnet50_42_TM+UANLL_95fd2a77-b348-4346-835c-57f4c8a3dfed_epoch=1.ckpt\n",
      "        optimizer_name: Adam\n",
      "        num_tta: 10\n",
      "        optimizer_hparams:\n",
      "          weight_decay: 0.0001\n",
      "        margin: 1.2\n",
      "      dataset:\n",
      "        name: isic_balanced\n",
      "        path: /repo/uncertainty_skin/data/isic_balanced/images\n",
      "        annotations_path: /repo/uncertainty_skin/data/isic_balanced/dataset.csv\n",
      "        img_size: 32\n",
      "        crop_scale:\n",
      "        - 0.8\n",
      "        - 1.0\n",
      "        crop_scale_tta:\n",
      "        - 0.8\n",
      "        - 1.0\n",
      "        batch_size: 32\n",
      "        num_workers: 0\n",
      "        target_name: target\n",
      "        class_names:\n",
      "        - benign\n",
      "        - malignant\n",
      "        train_size: 0.7\n",
      "        val_size: 0.15\n",
      "        test_size: 0.15\n",
      "        seed: 42\n",
      "        bagging: true\n",
      "        bagging_size: 15000\n",
      "        num_tta: 100\n",
      "        noise: 0.0\n",
      "        fixed: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\"configs\", job_name=\"test_app\"):\n",
    "    config = compose(config_name=\"config\", overrides=[\"+experiment=tm_ce_resnet\"])\n",
    "    print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 16:06:43,035 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "2024-12-24 16:06:43,036 DEBUG: Resetting dropped connection: huggingface.co\n",
      "2024-12-24 16:06:43,438 DEBUG: https://huggingface.co:443 \"HEAD /timm/resnet18.a1_in1k/resolve/main/model.safetensors HTTP/11\" 302 0\n",
      "2024-12-24 16:06:43,443 INFO: [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss is an additional loss term (module 1)\n"
     ]
    }
   ],
   "source": [
    "model = TimmModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "(15000, 8)\n",
      "target\n",
      "0.0    5361\n",
      "1.0    5139\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "0.0    1149\n",
      "1.0    1101\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "0.0    1149\n",
      "1.0    1101\n",
      "Name: count, dtype: int64\n",
      "Index([ 2442,  3224,   706, 12004,  9303,  5278, 13743, 13692,  4484,  4540,\n",
      "       ...\n",
      "       12959,  1779,   304, 14470,  3802,  1981, 10774,  3524,  9400, 12538],\n",
      "      dtype='int64', length=2250)\n"
     ]
    }
   ],
   "source": [
    "datamodule = ISICDataModule(config.dataset)\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 16:06:43,609 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "2024-12-24 16:06:43,791 DEBUG: https://huggingface.co:443 \"HEAD /timm/resnet18.a1_in1k/resolve/main/model.safetensors HTTP/11\" 302 0\n",
      "2024-12-24 16:06:43,792 INFO: [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2024-12-24 16:06:43,908 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss is an additional loss term (module 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 16:06:44,099 DEBUG: https://huggingface.co:443 \"HEAD /timm/resnet18.a1_in1k/resolve/main/model.safetensors HTTP/11\" 302 0\n",
      "2024-12-24 16:06:44,104 INFO: [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2024-12-24 16:06:44,215 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss is an additional loss term (module 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 16:06:44,407 DEBUG: https://huggingface.co:443 \"HEAD /timm/resnet18.a1_in1k/resolve/main/model.safetensors HTTP/11\" 302 0\n",
      "2024-12-24 16:06:44,412 INFO: [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2024-12-24 16:06:44,516 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss is an additional loss term (module 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 16:06:44,716 DEBUG: https://huggingface.co:443 \"HEAD /timm/resnet18.a1_in1k/resolve/main/model.safetensors HTTP/11\" 302 0\n",
      "2024-12-24 16:06:44,720 INFO: [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2024-12-24 16:06:44,822 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss is an additional loss term (module 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 16:06:45,006 DEBUG: https://huggingface.co:443 \"HEAD /timm/resnet18.a1_in1k/resolve/main/model.safetensors HTTP/11\" 302 0\n",
      "2024-12-24 16:06:45,007 INFO: [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss is an additional loss term (module 1)\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "# Load each checkpoint and store the model\n",
    "for checkpoint_path in config.model.checkpoint_path:\n",
    "    # Create a new instance of the model\n",
    "    model = TimmModel(config)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, weights_only=True)['state_dict'])  # Replace with your actual model class\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move the model to GPU\n",
    "    model = model.to('cuda')\n",
    "    \n",
    "    # Add the model to the list\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/uncertainty_skin/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Metrics:\n",
      "  Accuracy: Mean = 0.8052, Std = 0.1778\n",
      "  F1 Score: Mean = 0.8052, Std = 0.1778\n",
      "  ROC-AUC: Mean = 0.8469, Std = 0.1943\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Accuracy, F1Score, AUROC\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists to store metrics for individual models\n",
    "individual_accuracies = []\n",
    "individual_f1_scores = []\n",
    "individual_rocaucs = []\n",
    "\n",
    "# Test loop for individual models\n",
    "with torch.no_grad():\n",
    "    for i, model in enumerate(models):\n",
    "        # Reset metrics for each model\n",
    "        accuracy = Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
    "        f1 = F1Score(task=\"multiclass\", num_classes=2).to(device)\n",
    "        rocauc = AUROC(task=\"multiclass\", num_classes=2).to(device)\n",
    "        \n",
    "        for image, label in datamodule.test_dataloader():\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # Get predictions for the current model\n",
    "            predictions = model(image)\n",
    "            _, predicted_labels = torch.max(predictions, dim=1)\n",
    "            \n",
    "            # Update metrics\n",
    "            accuracy.update(predicted_labels, label)\n",
    "            f1.update(predicted_labels, label)\n",
    "            rocauc.update(predictions, label)  # ROC-AUC uses raw predictions\n",
    "        \n",
    "        # Compute and store metrics\n",
    "        individual_accuracies.append(accuracy.compute())\n",
    "        individual_f1_scores.append(f1.compute())\n",
    "        individual_rocaucs.append(rocauc.compute())\n",
    "\n",
    "# Compute mean and standard deviation for individual metrics\n",
    "mean_individual_accuracy = torch.mean(torch.tensor(individual_accuracies))\n",
    "std_individual_accuracy = torch.std(torch.tensor(individual_accuracies))\n",
    "\n",
    "mean_individual_f1 = torch.mean(torch.tensor(individual_f1_scores))\n",
    "std_individual_f1 = torch.std(torch.tensor(individual_f1_scores))\n",
    "\n",
    "mean_individual_rocauc = torch.mean(torch.tensor(individual_rocaucs))\n",
    "std_individual_rocauc = torch.std(torch.tensor(individual_rocaucs))\n",
    "\n",
    "print(f\"Individual Metrics:\")\n",
    "print(f\"  Accuracy: Mean = {mean_individual_accuracy:.4f}, Std = {std_individual_accuracy:.4f}\")\n",
    "print(f\"  F1 Score: Mean = {mean_individual_f1:.4f}, Std = {std_individual_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: Mean = {mean_individual_rocauc:.4f}, Std = {std_individual_rocauc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:07<00:00,  9.72it/s]\n",
      "100%|██████████| 71/71 [00:07<00:00,  9.92it/s]\n",
      "100%|██████████| 71/71 [00:07<00:00,  9.88it/s]\n",
      "100%|██████████| 71/71 [00:07<00:00,  9.85it/s]\n",
      "100%|██████████| 71/71 [00:07<00:00,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA Metrics (num_tta=10):\n",
      "  Accuracy: 0.9036\n",
      "  F1 Score: 0.9036\n",
      "  ROC-AUC: 0.9504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists to store metrics for TTA\n",
    "tta_accuracies = []\n",
    "tta_f1_scores = []\n",
    "tta_rocaucs = []\n",
    "\n",
    "#num_tta = config.dataset.num_tta\n",
    "num_tta = 10\n",
    "\n",
    "# Test loop for TTA\n",
    "with torch.no_grad():\n",
    "    for i, model in enumerate(models):\n",
    "        # Reset metrics for each model\n",
    "        tta_accuracy = Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
    "        tta_f1 = F1Score(task=\"multiclass\", num_classes=2).to(device)\n",
    "        tta_rocauc = AUROC(task=\"multiclass\", num_classes=2).to(device)\n",
    "        \n",
    "        for image, label in tqdm(datamodule.tta_dataloader()):\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # Initialize a tensor to store predictions for all TTA iterations\n",
    "            all_tta_predictions = []\n",
    "            \n",
    "            # Apply TTA for `num_tta` iterations\n",
    "            for _ in range(num_tta):\n",
    "                # Apply augmentation to the input image\n",
    "                # Get predictions for the current model (or ensemble)\n",
    "                predictions = model(image) \n",
    "                \n",
    "                # Store predictions\n",
    "                all_tta_predictions.append(predictions)\n",
    "            \n",
    "            # Stack predictions from all TTA iterations\n",
    "            all_tta_predictions = torch.stack(all_tta_predictions, dim=0)  # Shape: (num_tta, batch_size, num_classes)\n",
    "            \n",
    "            # Combine predictions (e.g., average them)\n",
    "            tta_prediction = torch.mean(all_tta_predictions, dim=0)  # Average over TTA iterations\n",
    "            \n",
    "            # Get the predicted class labels\n",
    "            _, predicted_labels = torch.max(tta_prediction, dim=1)\n",
    "            \n",
    "            # Update metrics\n",
    "            tta_accuracy.update(predicted_labels, label)\n",
    "            tta_f1.update(predicted_labels, label)\n",
    "            tta_rocauc.update(tta_prediction, label)  # ROC-AUC uses raw predictions\n",
    "\n",
    "# Compute final TTA metrics\n",
    "final_tta_accuracy = tta_accuracy.compute()\n",
    "final_tta_f1 = tta_f1.compute()\n",
    "final_tta_rocauc = tta_rocauc.compute()\n",
    "\n",
    "print(f\"TTA Metrics (num_tta={num_tta}):\")\n",
    "print(f\"  Accuracy: {final_tta_accuracy:.4f}\")\n",
    "print(f\"  F1 Score: {final_tta_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {final_tta_rocauc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "2024-12-24 16:07:34,948 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "2024-12-24 16:07:35,132 DEBUG: https://huggingface.co:443 \"HEAD /timm/resnet18.a1_in1k/resolve/main/model.safetensors HTTP/11\" 302 0\n",
      "2024-12-24 16:07:35,133 INFO: [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2024-12-24 16:07:35,252 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss is an additional loss term (module 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 16:07:35,436 DEBUG: https://huggingface.co:443 \"HEAD /timm/resnet18.a1_in1k/resolve/main/model.safetensors HTTP/11\" 302 0\n",
      "2024-12-24 16:07:35,437 INFO: [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2024-12-24 16:07:35,568 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss is an additional loss term (module 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 16:07:35,977 DEBUG: https://huggingface.co:443 \"HEAD /timm/resnet18.a1_in1k/resolve/main/model.safetensors HTTP/11\" 302 0\n",
      "2024-12-24 16:07:35,978 INFO: [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2024-12-24 16:07:36,100 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss is an additional loss term (module 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 16:07:36,368 DEBUG: https://huggingface.co:443 \"HEAD /timm/resnet18.a1_in1k/resolve/main/model.safetensors HTTP/11\" 302 0\n",
      "2024-12-24 16:07:36,369 INFO: [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2024-12-24 16:07:36,495 INFO: Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss is an additional loss term (module 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 16:07:36,676 DEBUG: https://huggingface.co:443 \"HEAD /timm/resnet18.a1_in1k/resolve/main/model.safetensors HTTP/11\" 302 0\n",
      "2024-12-24 16:07:36,677 INFO: [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss is an additional loss term (module 1)\n",
      "Ensemble Metrics:\n",
      "  Accuracy: 0.9160\n",
      "  F1 Score: 0.9160\n",
      "  ROC-AUC: 0.9605\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.func import stack_module_state, functional_call\n",
    "import copy\n",
    "from torchmetrics import Accuracy \n",
    "\n",
    "# List to store individual models\n",
    "pl.seed_everything(42)\n",
    "\n",
    "models = []\n",
    "\n",
    "# Load each checkpoint and store the model\n",
    "for checkpoint_path in config.model.checkpoint_path:\n",
    "    # Create a new instance of the model\n",
    "    model = TimmModel(config)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, weights_only=True)['state_dict'])  # Replace with your actual model class\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move the model to GPU\n",
    "    model = model.to('cuda')\n",
    "    \n",
    "    # Add the model to the list\n",
    "    models.append(model)\n",
    "\n",
    "# Step 1: Stack the states of all models\n",
    "params, buffers = stack_module_state(models)\n",
    "\n",
    "# Move stacked parameters and buffers to GPU\n",
    "params = {k: v.to('cuda') for k, v in params.items()}\n",
    "buffers = {k: v.to('cuda') for k, v in buffers.items()}\n",
    "\n",
    "# Step 2: Create a stateless version of the model\n",
    "base_model = copy.deepcopy(models[0])\n",
    "base_model = base_model.to('meta')\n",
    "\n",
    "# Step 3: Define a functional model\n",
    "def fmodel(params, buffers, x):\n",
    "    return functional_call(base_model, (params, buffers), (x,))\n",
    "\n",
    "# Step 4: Use vmap to vectorize the application of the functional model\n",
    "# Option 1: Use the same minibatch for all models\n",
    "def ensemble_predictions(predictions):\n",
    "    return torch.mean(predictions, dim=0)\n",
    "\n",
    "# Move dataloader to GPU\n",
    "device = 'cuda'\n",
    "\n",
    "# Initialize accuracy metric\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
    "\n",
    "# Initialize metrics for ensemble\n",
    "ensemble_accuracy = Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
    "ensemble_f1 = F1Score(task=\"multiclass\", num_classes=2).to(device)\n",
    "ensemble_rocauc = AUROC(task=\"multiclass\", num_classes=2).to(device)\n",
    "\n",
    "# Test loop for ensemble\n",
    "with torch.no_grad():\n",
    "    for image, label in datamodule.test_dataloader():\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        # Apply all models to the input image\n",
    "        predictions = torch.vmap(fmodel, in_dims=(0, 0, None))(params, buffers, image)\n",
    "        \n",
    "        # Combine predictions (e.g., average them)\n",
    "        ensemble_prediction = ensemble_predictions(predictions)\n",
    "        \n",
    "        # Get the predicted class labels\n",
    "        _, predicted_labels = torch.max(ensemble_prediction, dim=1)\n",
    "        \n",
    "        # Update metrics\n",
    "        ensemble_accuracy.update(predicted_labels, label)\n",
    "        ensemble_f1.update(predicted_labels, label)\n",
    "        ensemble_rocauc.update(ensemble_prediction, label)  # ROC-AUC uses raw predictions\n",
    "\n",
    "# Compute final ensemble metrics\n",
    "final_ensemble_accuracy = ensemble_accuracy.compute()\n",
    "final_ensemble_f1 = ensemble_f1.compute()\n",
    "final_ensemble_rocauc = ensemble_rocauc.compute()\n",
    "\n",
    "print(f\"Ensemble Metrics:\")\n",
    "print(f\"  Accuracy: {final_ensemble_accuracy:.4f}\")\n",
    "print(f\"  F1 Score: {final_ensemble_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {final_ensemble_rocauc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/71 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (32) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m _, predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(tta_prediction, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Update metrics\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mtta_accuracy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m tta_f1\u001b[38;5;241m.\u001b[39mupdate(predicted_labels\u001b[38;5;241m.\u001b[39mfloat(), label)\n\u001b[1;32m     51\u001b[0m tta_rocauc\u001b[38;5;241m.\u001b[39mupdate(tta_prediction, label)  \u001b[38;5;66;03m# ROC-AUC uses raw predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/uncertainty_skin/lib/python3.12/site-packages/torchmetrics/metric.py:493\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    487\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m device corresponds to the device of the input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    492\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_on_cpu:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_list_states_to_cpu()\n",
      "File \u001b[0;32m~/miniconda/envs/uncertainty_skin/lib/python3.12/site-packages/torchmetrics/metric.py:483\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/miniconda/envs/uncertainty_skin/lib/python3.12/site-packages/torchmetrics/classification/stat_scores.py:343\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    339\u001b[0m     _multiclass_stat_scores_tensor_validation(\n\u001b[1;32m    340\u001b[0m         preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    341\u001b[0m     )\n\u001b[1;32m    342\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[0;32m--> 343\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m \u001b[43m_multiclass_stat_scores_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state(tp, fp, tn, fn)\n",
      "File \u001b[0;32m~/miniconda/envs/uncertainty_skin/lib/python3.12/site-packages/torchmetrics/functional/classification/stat_scores.py:401\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_update\u001b[0;34m(preds, target, num_classes, top_k, average, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    399\u001b[0m     preds \u001b[38;5;241m=\u001b[39m preds[idx]\n\u001b[1;32m    400\u001b[0m     target \u001b[38;5;241m=\u001b[39m target[idx]\n\u001b[0;32m--> 401\u001b[0m tp \u001b[38;5;241m=\u001b[39m (\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    402\u001b[0m fp \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m!=\u001b[39m target)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    403\u001b[0m fn \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m!=\u001b[39m target)\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (32) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists to store metrics for TTA\n",
    "tta_accuracies = []\n",
    "tta_f1_scores = []\n",
    "tta_rocaucs = []\n",
    "\n",
    "#num_tta = config.dataset.num_tta\n",
    "num_tta = 10\n",
    "\n",
    "# Test loop for TTA\n",
    "with torch.no_grad():\n",
    "    for i, model in enumerate(models):\n",
    "        # Reset metrics for each model\n",
    "        tta_accuracy = Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
    "        tta_f1 = F1Score(task=\"multiclass\", num_classes=2).to(device)\n",
    "        tta_rocauc = AUROC(task=\"multiclass\", num_classes=2).to(device)\n",
    "        \n",
    "        for image, label in tqdm(datamodule.tta_dataloader()):\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # Initialize a tensor to store predictions for all TTA iterations\n",
    "            all_tta_predictions = []\n",
    "            \n",
    "            # Apply TTA for `num_tta` iterations\n",
    "            for _ in range(num_tta):\n",
    "                # Apply all models to the input image\n",
    "                predictions = torch.vmap(fmodel, in_dims=(0, 0, None))(params, buffers, image)\n",
    "                \n",
    "                # Combine predictions (e.g., average them)\n",
    "                ensemble_prediction = ensemble_predictions(predictions)\n",
    "                \n",
    "                ensemble_prediction = torch.mean(predictions, dim=0)\n",
    "                \n",
    "                # Store predictions\n",
    "                all_tta_predictions.append(predictions)\n",
    "            \n",
    "            # Stack predictions from all TTA iterations\n",
    "            all_tta_predictions = torch.stack(all_tta_predictions, dim=0)  # Shape: (num_tta, batch_size, num_classes)\n",
    "            \n",
    "            # Combine predictions (e.g., average them)\n",
    "            tta_prediction = torch.mean(all_tta_predictions, dim=0).float()  # Average over TTA iterations\n",
    "            \n",
    "            # Get the predicted class labels\n",
    "            _, predicted_labels = torch.max(tta_prediction, dim=1)\n",
    "            \n",
    "            # Update metrics\n",
    "            tta_accuracy.update(predicted_labels.float(), label)\n",
    "            tta_f1.update(predicted_labels.float(), label)\n",
    "            tta_rocauc.update(tta_prediction, label)  # ROC-AUC uses raw predictions\n",
    "\n",
    "# Compute final TTA metrics\n",
    "final_tta_accuracy = tta_accuracy.compute()\n",
    "final_tta_f1 = tta_f1.compute()\n",
    "final_tta_rocauc = tta_rocauc.compute()\n",
    "\n",
    "print(f\"TTA Metrics (num_tta={num_tta}):\")\n",
    "print(f\"  Accuracy: {final_tta_accuracy:.4f}\")\n",
    "print(f\"  F1 Score: {final_tta_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {final_tta_rocauc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncertainty_skin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
